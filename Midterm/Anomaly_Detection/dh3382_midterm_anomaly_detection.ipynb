{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df96bd9-3a00-4406-9cf7-61015eef3f52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "1. For each model, compute the bi-monthly failure rate R per model: F = number of failures per model, O = number of cumulative days in operation per model, D = number of days between Jan 1, 2019 and March 28, 2019, inclusive:\n",
    "\n",
    "\n",
    "\\\\[R = 100.0 * \\left(\\frac{1.0 * F}{O \\div D}\\right)\\\\]\n",
    "\n",
    "\n",
    "2. Given R per model, find the mean M and standard deviation S\n",
    "\n",
    "3. Use M, S to predict which models in operation on March 29, 2019 will fail, with failure\n",
    "predicted if the modelâ€™s R exceeds M + 1S\n",
    "\n",
    "Log data from Backblaze\\\n",
    "Source: https://www.backblaze.com/blog/backblaze-hard-drive-stats-q1-2019/ \\\n",
    "Reference: https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ed5b21-0139-4f61-a628-4d0300c19329",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "spark = SparkSession.builder.appName('dh3382-midterm-anomaly-detection').getOrCreate()\n",
    "\n",
    "LOG_DIR_PATH = '/FileStore/tables/drive_stats_2019_Q1'\n",
    "\n",
    "# specified date range\n",
    "DATE_RANGE = ('2019-1-1', '2019-3-28')\n",
    "\n",
    "# calculate number of days in date range for D variable in failure rate (R)\n",
    "DATE_RANGE_DAYS = (dt.strptime(DATE_RANGE[1], \"%Y-%m-%d\") - dt.strptime(DATE_RANGE[0], \"%Y-%m-%d\")).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b00fa36-0d7b-45e4-a137-d6e3b98c152a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------+-------+\n|      date|serial_number|      model|failure|\n+----------+-------------+-----------+-------+\n|2019-01-08|     Z305B2QN|ST4000DM000|      0|\n|2019-01-09|     Z305B2QN|ST4000DM000|      0|\n|2019-01-18|     Z305B2QN|ST4000DM000|      0|\n|2019-01-19|     Z305B2QN|ST4000DM000|      0|\n|2019-01-20|     Z305B2QN|ST4000DM000|      0|\n|2019-01-21|     Z305B2QN|ST4000DM000|      0|\n|2019-01-22|     Z305B2QN|ST4000DM000|      0|\n|2019-01-23|     Z305B2QN|ST4000DM000|      0|\n|2019-01-24|     Z305B2QN|ST4000DM000|      0|\n|2019-01-25|     Z305B2QN|ST4000DM000|      0|\n|2019-01-26|     Z305B2QN|ST4000DM000|      0|\n|2019-01-27|     Z305B2QN|ST4000DM000|      0|\n|2019-01-30|     Z305B2QN|ST4000DM000|      0|\n|2019-01-31|     Z305B2QN|ST4000DM000|      0|\n|2019-02-04|     Z305B2QN|ST4000DM000|      0|\n|2019-02-10|     Z305B2QN|ST4000DM000|      0|\n|2019-03-08|     Z305B2QN|ST4000DM000|      0|\n|2019-03-09|     Z305B2QN|ST4000DM000|      0|\n|2019-03-18|     Z305B2QN|ST4000DM000|      0|\n|2019-03-19|     Z305B2QN|ST4000DM000|      0|\n|2019-03-20|     Z305B2QN|ST4000DM000|      0|\n|2019-03-21|     Z305B2QN|ST4000DM000|      0|\n|2019-03-22|     Z305B2QN|ST4000DM000|      0|\n|2019-03-23|     Z305B2QN|ST4000DM000|      0|\n|2019-03-24|     Z305B2QN|ST4000DM000|      0|\n|2019-03-25|     Z305B2QN|ST4000DM000|      0|\n|2019-03-26|     Z305B2QN|ST4000DM000|      0|\n|2019-03-27|     Z305B2QN|ST4000DM000|      0|\n+----------+-------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# read in raw log data\n",
    "log_df_raw = spark.read.options(inferSchema=True, header=True).csv(LOG_DIR_PATH)\n",
    "\n",
    "# select only necessary columns\n",
    "log_df = log_df_raw.select('date', 'serial_number', 'model', 'failure')\n",
    "\n",
    "# filter entries outside of specified date range\n",
    "log_df = log_df.filter(log_df.date.between(*DATE_RANGE))\n",
    "\n",
    "log_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dh3382_midterm_anomaly_detection",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
