{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd1433e-46a6-4320-8d91-0ddcbea7321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run if haversine not already installed\n",
    "# %pip install haversine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998fd923-3d97-4f77-9211-4fc7195b930e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, count, dayofweek, explode, hour, lit, lower, regexp_replace, row_number, size, split, sum, udf\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine as hs\n",
    "\n",
    "spark = SparkSession.builder.appName(\"dh3382-hw2\").getOrCreate()\n",
    "BAKERY_PATH = '/FileStore/tables/BreadBasket_DMS.csv'\n",
    "POP_PATH = '/FileStore/tables/populationbycountry19802010millions.csv'\n",
    "REST_PATH = '/FileStore/tables/Restaurants_in_Durham_County_NC.csv'\n",
    "REST_FORECLOSED_PATH = '/FileStore/tables/durham_nc_foreclosure_2006_2016.json'\n",
    "WORD_DIR_PATH = '/FileStore/tables/hw1text/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42dbdc2d-6585-496c-974b-9828bb5f258c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read and clean bakery data\n",
    "bakery_data = spark.read.options(header=True, inferSchema=True).csv(BAKERY_PATH)\n",
    "\n",
    "# remove NONE entries\n",
    "bakery_data = bakery_data.filter(col('Item') != 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f04f4d7-2566-4ebe-956b-7bcbc849121a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+\n|             Item|      Date|Quantity|\n+-----------------+----------+--------+\n|         Focaccia|2016-11-03|       1|\n|          Tartine|2016-11-04|       1|\n|            Bread|2016-12-13|       4|\n|           Coffee|2017-01-05|       7|\n|     Scandinavian|2017-01-20|       1|\n|         Art Tray|2017-01-24|       1|\n|            Bread|2017-03-22|       6|\n| Coffee granules |2017-03-25|       1|\n|           Muffin|2016-11-14|       1|\n|           Coffee|2016-11-24|       7|\n|Gingerbread syrup|2016-12-21|       1|\n|           Coffee|2017-01-06|       8|\n|     Scandinavian|2017-01-07|       3|\n|            Bread|2017-01-11|       6|\n|           Coffee|2017-02-01|       4|\n|     Chicken Stew|2017-02-08|       1|\n|    Hot chocolate|2017-02-09|       1|\n|       Farm House|2017-02-12|       1|\n|          Brownie|2017-02-13|       1|\n|            Bread|2017-02-27|       5|\n+-----------------+----------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 1: Show the total number bought by item, per day, between 11AM and 1PM #####\n",
    "\n",
    "# filter out all transactions outside of specified time range (11:00-13:00 inclusive)\n",
    "bakery_data_lunch = bakery_data.filter(col('Time').between('11:00:00', '13:00:00'))\n",
    "\n",
    "# group by Item and Day purchased, then get total count of items per day purchased between 11:00-13:00\n",
    "bakery_q1_res = bakery_data_lunch.groupBy(col('Item'), col('Date')).count().withColumnRenamed('count', 'Quantity')\n",
    "\n",
    "# DF with answer to question 1\n",
    "bakery_q1_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f13d60-b5dd-402b-93fb-d60745d5cce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 2: Show the top 3 (by qty) items bought by Daypart, by DayType #####\n",
    "#udf to define weekend/weekday\n",
    "def weekend(is_weekend):\n",
    "    if is_weekend:\n",
    "        return \"Weekend\"\n",
    "    else:\n",
    "        return \"Weekday\"\n",
    "\n",
    "weekend_udf = udf(lambda is_weekend: weekend(is_weekend), StringType() )\n",
    "\n",
    "\n",
    "# udf to define part of day\n",
    "def day_part(date):\n",
    "    if (date.hour >= 7) & (date.hour < 12):\n",
    "        return \"Morning\"\n",
    "    elif date.hour < 17:\n",
    "        return \"Afternoon\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "day_part_udf = udf(lambda hour: day_part(hour), StringType() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a12208-7089-45b1-a30e-e11b6615907b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-------+\n|Daypart  |Top_3_Items                               |Daytype|\n+---------+------------------------------------------+-------+\n|Afternoon|[Coffee, Bread, Tea]                      |Weekday|\n|Afternoon|[Coffee, Bread, Tea]                      |Weekend|\n|Morning  |[Coffee, Bread, Pastry]                   |Weekday|\n|Morning  |[Coffee, Bread, Pastry]                   |Weekend|\n|Night    |[Coffee, Bread, Tea]                      |Weekday|\n|Night    |[Coffee, Tshirt, Afternoon with the baker]|Weekend|\n+---------+------------------------------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# use weekend udf to add Daytype column, pass in boolean with sparkSQL dayofweek and isin functions\n",
    "bakery_weekend = bakery_data.withColumn('Daytype', weekend_udf(dayofweek(col('Date') ).isin([1, 7]) ) )\n",
    "\n",
    "# use day_part udf to add Daypart column\n",
    "bakery_daypart = bakery_weekend.withColumn('Daypart', day_part_udf(col('Time') ) )\n",
    "\n",
    "#group by Daypart and Daytype and count purchases\n",
    "bakery_purchases = bakery_daypart.groupBy(col('Daypart'), col(\"Daytype\"), col('Item') ).count().withColumnRenamed('count', 'Purchases')\n",
    "\n",
    "# partition by Daypart, Daytype and find top 3 Purchases for each Daypart/Daytime combination\n",
    "windowBakery = Window.partitionBy('Daypart', 'Daytype').orderBy(col('Purchases').desc() )\n",
    "bakery_top_purchases = bakery_purchases.withColumn(\"row\",row_number()\\\n",
    "  .over(windowBakery) ) \\\n",
    "  .filter(col(\"row\") <= 3) \\\n",
    "\n",
    "# Collect all items into single row grouped by Daypart/Daytype\n",
    "bakery_q2_res = bakery_top_purchases.groupBy(col('Daypart'), col(\"Daytype\") ).agg(collect_list('Item').alias('Top_3_Items') )\n",
    "\n",
    "# Select columns in order Daypart/Top_3_Items/Daytype for cleaner output format\n",
    "bakery_q2_res.select(col('Daypart'), col('Top_3_Items'), col('Daytype') ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f934241-835d-4c57-ae74-65c5f225b7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 3: The total number of entities by â€œrpt_area_descâ€ #####\n",
    "# set European style csv delimiter (;)\n",
    "rest_data_raw = spark.read.options(header=True, delimiter=';', inferSchema=True)\\\n",
    "    .csv(REST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083bd6c0-d148-4e1b-a589-f1bebf6d09ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n| Rpt_Area_Desc|Total|\n+--------------+-----+\n|  Food Service| 1093|\n|Swimming Pools|  420|\n|   Summer Food|  242|\n+--------------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# group by rpt area desc, aggregate count as 'Total', then sort in descending order and show top 3\n",
    "rest_q3_res = rest_data_raw.groupBy(col('Rpt_Area_Desc') )\\\n",
    "    .agg(count('*').alias('Total') )\\\n",
    "    .sort(col('Total'), ascending=False)\\\n",
    "\n",
    "rest_q3_res.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f712ce0b-b562-448c-84ea-3f826f8306af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####  QUESTION 4: Show the top 10 regions with the biggest percentage decrease in population, for the years 1990-2000 #####\n",
    "\n",
    "pop_data_raw = spark.read.options(header=True, inferSchema=True)\\\n",
    "    .csv(POP_PATH)\n",
    "\n",
    "# first column name is Null in file, find col name through columns attribute and rename first column to region\n",
    "cols = pop_data_raw.columns \n",
    "pop_data_clean_header = pop_data_raw.select(col(cols[0]).alias('Region'), col('1990'), col('2000') )\n",
    "\n",
    "# remove regions with no population data (e.g. Antarctica, Wake Island, Croatia, Former U.S.S.R.)\n",
    "pop_data_clean_pop_nums = pop_data_clean_header.filter((col('1990')  != 'NA') &  (col('2000')  != 'NA') )\n",
    "pop_data_clean_pop_nums = pop_data_clean_pop_nums.filter((col('1990')  != '--') &  (col('2000')  != '--') )\n",
    "\n",
    "# remove leftover aggregate regions\n",
    "pop_data_clean_regions = pop_data_clean_pop_nums\\\n",
    "    .filter((col('Region') != 'World') & (col('Region') != 'North America') & (col('Region') != 'Central & South America') & (col('Region') != 'Eurasia') & (col('Region') != 'Western Sahara') & (col('Region') != 'Asia & Oceania') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22854c04-f33b-490f-ba43-5011724e59f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n|       Region|       perc_increase|\n+-------------+--------------------+\n|   Montserrat| -0.6318732525629077|\n|     Bulgaria|-0.12092718374010437|\n| Cook Islands|-0.11310494834148986|\n| Sierra Leone|-0.09912965328035572|\n|       Kuwait|-0.07841983884671756|\n|    Gibraltar| -0.0632085194091378|\n|Faroe Islands|-0.03478077571669489|\n|      Albania|-0.02668162333239...|\n|      Hungary|-0.02164024265610...|\n|      Romania|-0.01830669620112...|\n+-------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# calculate gross increase\n",
    "pop_decrease_gross = pop_data_clean_regions.withColumn('gross_increase', col('2000') - col('1990') )\n",
    "\n",
    "# remove regions with positive population growth\n",
    "pop_decrease_gross = pop_decrease_gross.filter(col('gross_increase') <= 0)\n",
    "\n",
    "# calculate percentage of growth increase\n",
    "pop_decrease_perc = pop_decrease_gross.withColumn('perc_increase', col('gross_increase')/col('1990') )\n",
    "\n",
    "# create DF sorted in ascending order of pop increase, only contains Region and perc_increase \n",
    "pop_q4_res = pop_decrease_perc.sort(col('perc_increase') )\\\n",
    "    .select(col('Region'), col('perc_increase') )\n",
    "\n",
    "# show top 10 results\n",
    "pop_q4_res.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b42a86-d1fe-45bf-926c-b705df5fa7d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 5 #####\n",
    "\"\"\"\n",
    "Do word count exercise using pyspark. Ignore punctuation and normalize to lower case. Replace characters NOT in this set: [0-9a-z] with space.\n",
    "\"\"\"\n",
    "word_df = spark.read.text(WORD_DIR_PATH)\n",
    "\n",
    "# normalize to lower case\n",
    "word_df_lower = word_df.select(lower(col('value')).alias('words') )\n",
    "\n",
    "# replace punctuation with space\n",
    "word_df_parsed = word_df_lower.withColumn('words', regexp_replace('words', '[^a-z0-9]', ' ') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c30b6e-1af8-413f-aa16-40c9d23fbf50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|       words|count|\n+------------+-----+\n|       trail|   57|\n|       those| 3409|\n|    medicare|   32|\n|        some| 4335|\n|         few| 1057|\n|   connected|  162|\n| herzegovina|    7|\n|   involving|   99|\n|    randomly|   10|\n|     clinics|   78|\n|       still| 2139|\n| transmitted|   92|\n|      travel| 1367|\n|vicissitudes|    1|\n|      online| 1357|\n|         wto|   17|\n|  paramedics|   26|\n|          07|   90|\n|   traveling|   97|\n|   recognize|   66|\n+------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# explode and overwrite words column to create row for each word\n",
    "word_df_explode = word_df_parsed.withColumn('words', explode(split('words', \" +\") ) )\n",
    "\n",
    "# group by words and count each occurence of each word\n",
    "word_q5_res = word_df_explode.groupBy(col('words') ).count()\n",
    "\n",
    "# show result\n",
    "word_q5_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3f43d1-c099-42ac-9390-bcde5a033825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n| bigrams|count|\n+--------+-----+\n|  of the|17484|\n|  in the|12808|\n|   p the|10363|\n|covid 19| 8762|\n|  to the| 8372|\n| for the| 5588|\n|     n t| 5393|\n|  on the| 5032|\n|   to be| 4581|\n| will be| 4177|\n+--------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 6: Find the 10 most common bigrams #####\n",
    "\n",
    "# convert strings to arrays of strings for NGram transformation\n",
    "word_df_str_arrays = word_df_parsed.withColumn('words', split('words', ' +') )\n",
    "\n",
    "# transform word_df to bigram_df using NGram feature\n",
    "bigram = NGram(n=2).setInputCol('words').setOutputCol('bigrams')\n",
    "bigram_df = bigram.transform(word_df_str_arrays)\n",
    "\n",
    "# remove unnecessary words column\n",
    "bigram_df = bigram_df.select(col('bigrams') )\n",
    "\n",
    "# explode bigrams so each has own separate row\n",
    "bigram_df = bigram_df.withColumn('bigrams', explode(col('bigrams') ) )\n",
    "\n",
    "# group by bigram and count occurences\n",
    "bigram_df = bigram_df.groupBy(col('bigrams') ).count()\n",
    "\n",
    "# sort on bigram count for q5 res\n",
    "bigram_q5_res = bigram_df.sort('count', ascending=False)\n",
    "\n",
    "# show top 10 bigrams\n",
    "bigram_q5_res.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce430b64-3979-4b65-ab1e-08b562fa527a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Question 7 #####\n",
    "\"\"\"\n",
    "a) Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\n",
    "b) With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\n",
    "\"\"\"\n",
    "# user function that accepts 2 strings of coordinates and outputs float distance in miles for part a\n",
    "def hs_with_str(coord_str1, coord_str2):\n",
    "    # convert coordinate strings to float tuples\n",
    "    coord_tup1 = tuple(map(float, coord_str1.split(', ') ) )\n",
    "    coord_tup2 = tuple(map(float, coord_str2.split(', ') ) )\n",
    "\n",
    "    # plug float tuples intro haversine function, specify unit as miles\n",
    "    return hs(coord_tup1, coord_tup2, unit='mi')\n",
    "\n",
    "# convert hs_with_str to udf\n",
    "hs_with_str_udf = udf(lambda coord1, coord2: hs_with_str(coord1, coord2), FloatType() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f02bdc2-e49d-46f5-8a78-94b8de328e82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-------------+-----------------------+------------+\n|Premise_Name            |Status|Rpt_Area_Desc|geolocation            |Distance(mi)|\n+------------------------+------+-------------+-----------------------+------------+\n|OLD HAVANA SANDWICH SHOP|ACTIVE|Food Service |35.9932826, -78.8981331|0.1258222   |\n+------------------------+------+-------------+-----------------------+------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "## PART A: Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\n",
    "\n",
    "# origin coordinate string specified in prompt\n",
    "ORIGIN_COORD_STR = '35.994914, -78.897133'\n",
    "\n",
    "#filter dataset to include only active food service restaurants\n",
    "rest_data_active_fs = rest_data_raw.filter(col('Status') == 'ACTIVE').filter(col('Rpt_Area_Desc') == 'Food Service')\n",
    "\n",
    "# drop any restaurants with no geolocation data\n",
    "rest_data_active_fs = rest_data_active_fs.na.drop(subset=['geolocation'])\n",
    "\n",
    "# apply haversine UDF to create Distance(mi) column\n",
    "rest_data_distance = rest_data_active_fs.withColumn('Distance(mi)', hs_with_str_udf(col('geolocation'), lit(ORIGIN_COORD_STR) ) )\n",
    "\n",
    "# sort on Distance(mi) for final result\n",
    "rest_q7a_res = rest_data_distance.sort('Distance(mi)')\n",
    "\n",
    "# show closest restaurant, select only relevant columns for cleaner output\n",
    "rest_q7a_res\\\n",
    "    .select(col('Premise_Name'), col('Status'), col('Rpt_Area_Desc'), col('geolocation'), col('Distance(mi)') )\\\n",
    "    .show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cf25b5-63f0-4865-83fd-8b192d508e47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## PART B: With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\n",
    "\n",
    "# save coordinates of restaurant from Q7a to variable COORD_STR\n",
    "COORD_STR = rest_q7a_res.collect()[0]['geolocation']\n",
    "\n",
    "# read in foreclosure dataset\n",
    "fc_rest_raw = spark.read.json(REST_FORECLOSED_PATH)\n",
    "\n",
    "# select only geocode subfield and discard unnecessary data\n",
    "fc_rest_coords = fc_rest_raw.select(col('fields').getItem('geocode').alias('geocodes') )\n",
    "\n",
    "# remove entries with null geolocation values\n",
    "fc_rest_coords = fc_rest_coords.na.drop()\n",
    "\n",
    "# cast geocode column to string\n",
    "fc_coords_str = fc_rest_coords.withColumn('geocodes', col('geocodes').cast(StringType() ) )\n",
    "\n",
    "# remove brackets\n",
    "fc_coords_str = fc_coords_str.withColumn('geocodes', regexp_replace('geocodes', \"\\[\", \"\") )\n",
    "fc_coords_str = fc_coords_str.withColumn('geocodes', regexp_replace('geocodes', \"\\]\", \"\") )\n",
    "\n",
    "# calculate Distance(mi) column using hs_with_str_udf\n",
    "fc_coords_distance = fc_coords_str.withColumn('Distance(mi)', hs_with_str_udf(col('geocodes'), lit(COORD_STR) ) )\n",
    "\n",
    "#filter out all restaurants more than 1 mile from center point\n",
    "fc_coords_q7b_res = fc_coords_distance.filter(col('Distance(mi)') <= 1)\n",
    "\n",
    "# count and print umber of foreclosures within 1 mile of center point\n",
    "print(\"RESULT\")\n",
    "print('Number of foreclosures within 1 mile of center point: %d' % fc_coords_q7b_res.count() )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dh3382-hw2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
