{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd1433e-46a6-4320-8d91-0ddcbea7321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: haversine in /local_disk0/.ephemeral_nfs/envs/pythonEnv-dfb68fb3-d0b7-41cf-9b09-2a9d28162701/lib/python3.9/site-packages (2.8.0)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998fd923-3d97-4f77-9211-4fc7195b930e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, count, dayofweek, explode, hour, lit, lower, regexp_replace, row_number, size, split, sum, udf\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine as hs\n",
    "from haversine import Unit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"dh3382-hw2\").getOrCreate()\n",
    "BAKERY_PATH = '/FileStore/tables/BreadBasket_DMS.csv'\n",
    "REST_PATH = '/FileStore/tables/Restaurants_in_Durham_County_NC.csv'\n",
    "POP_PATH = '/FileStore/tables/populationbycountry19802010millions.csv'\n",
    "WORD_DIR_PATH = '/FileStore/tables/hw1text/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42dbdc2d-6585-496c-974b-9828bb5f258c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read and clean bakery data\n",
    "bakery_data = spark.read.options(header=True, inferSchema=True).csv(BAKERY_PATH)\n",
    "# remove NONE entries\n",
    "bakery_data = bakery_data.filter(col('Item') != 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f04f4d7-2566-4ebe-956b-7bcbc849121a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+--------+\n|             Item|      Date|Quantity|\n+-----------------+----------+--------+\n|         Focaccia|2016-11-03|       1|\n|          Tartine|2016-11-04|       1|\n|            Bread|2016-12-13|       4|\n|           Coffee|2017-01-05|       7|\n|     Scandinavian|2017-01-20|       1|\n|         Art Tray|2017-01-24|       1|\n|            Bread|2017-03-22|       6|\n| Coffee granules |2017-03-25|       1|\n|           Muffin|2016-11-14|       1|\n|           Coffee|2016-11-24|       7|\n|Gingerbread syrup|2016-12-21|       1|\n|           Coffee|2017-01-06|       8|\n|     Scandinavian|2017-01-07|       3|\n|            Bread|2017-01-11|       6|\n|           Coffee|2017-02-01|       4|\n|     Chicken Stew|2017-02-08|       1|\n|    Hot chocolate|2017-02-09|       1|\n|       Farm House|2017-02-12|       1|\n|          Brownie|2017-02-13|       1|\n|            Bread|2017-02-27|       5|\n+-----------------+----------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 1: Show the total number bought by item, per day, between 11AM and 1PM #####\n",
    "\n",
    "# filter out all transactions outside of specified time range (11:00-13:00 inclusive)\n",
    "bakery_data_lunch = bakery_data.filter(col('Time').between('11:00:00', '13:00:00'))\n",
    "# group by Item and Day purchased, then get total count of items per day purchased between 11:00-13:00\n",
    "bakery_q1_res = bakery_data_lunch.groupBy(col('Item'), col('Date')).count().withColumnRenamed('count', 'Quantity')\n",
    "# DF with answer to question 1\n",
    "bakery_q1_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f13d60-b5dd-402b-93fb-d60745d5cce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 2: Show the top 3 (by qty) items bought by Daypart, by DayType #####\n",
    "#udf to define weekend/weekday\n",
    "def weekend(is_weekend):\n",
    "    if is_weekend:\n",
    "        return \"Weekend\"\n",
    "    else:\n",
    "        return \"Weekday\"\n",
    "\n",
    "weekend_udf = udf(lambda is_weekend: weekend(is_weekend), StringType() )\n",
    "\n",
    "\n",
    "# udf to define part of day\n",
    "def day_part(date):\n",
    "    if (date.hour >= 7) & (date.hour < 12):\n",
    "        return \"Morning\"\n",
    "    elif date.hour < 17:\n",
    "        return \"Afternoon\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "day_part_udf = udf(lambda hour: day_part(hour), StringType() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a12208-7089-45b1-a30e-e11b6615907b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-------+\n|Daypart  |Top_3_Items                               |Daytype|\n+---------+------------------------------------------+-------+\n|Afternoon|[Coffee, Bread, Tea]                      |Weekday|\n|Afternoon|[Coffee, Bread, Tea]                      |Weekend|\n|Morning  |[Coffee, Bread, Pastry]                   |Weekday|\n|Morning  |[Coffee, Bread, Pastry]                   |Weekend|\n|Night    |[Coffee, Bread, Tea]                      |Weekday|\n|Night    |[Coffee, Tshirt, Afternoon with the baker]|Weekend|\n+---------+------------------------------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# use weekend udf to add Daytype column, pass in boolean with sparkSQL dayofweek and isin functions\n",
    "bakery_weekend = bakery_data.withColumn('Daytype', weekend_udf(dayofweek(col('Date') ).isin([1, 7]) ) )\n",
    "# use day_part udf to add Daypart column\n",
    "bakery_daypart = bakery_weekend.withColumn('Daypart', day_part_udf(col('Time') ) )\n",
    "#group by Daypart and Daytype and count purchases\n",
    "bakery_purchases = bakery_daypart.groupBy(col('Daypart'), col(\"Daytype\"), col('Item') ).count().withColumnRenamed('count', 'Purchases')\n",
    "\n",
    "# partition by Daypart, Daytype and find top 3 Purchases for each Daypart/Daytime combination\n",
    "windowBakery = Window.partitionBy('Daypart', 'Daytype').orderBy(col('Purchases').desc() )\n",
    "bakery_top_purchases = bakery_purchases.withColumn(\"row\",row_number()\\\n",
    "  .over(windowBakery)) \\\n",
    "  .filter(col(\"row\") <= 3) \\\n",
    "\n",
    "# Collect all items into single row grouped by Daypart/Daytype\n",
    "bakery_q2_res = bakery_top_purchases.groupBy(col('Daypart'), col(\"Daytype\")).agg(collect_list('Item').alias('Top_3_Items') )\n",
    "# Select columns in order Daypart/Top_3_Items/Daytype for cleaner output format\n",
    "bakery_q2_res.select(col('Daypart'), col('Top_3_Items'), col('Daytype') ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f934241-835d-4c57-ae74-65c5f225b7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 3: The total number of entities by “rpt_area_desc” #####\n",
    "# set European style csv delimiter (;)\n",
    "rest_data_raw = spark.read.options(header=True, delimiter=';', inferSchema=True)\\\n",
    "    .csv(REST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083bd6c0-d148-4e1b-a589-f1bebf6d09ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n| Rpt_Area_Desc|Total|\n+--------------+-----+\n|  Food Service| 1093|\n|Swimming Pools|  420|\n|   Summer Food|  242|\n+--------------+-----+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# group by rpt area desc, aggregate count as 'Total', then sort in descending order and show top 3\n",
    "rest_q3_res = rest_data_raw.groupBy(col('Rpt_Area_Desc'))\\\n",
    "    .agg(count('*').alias('Total') )\\\n",
    "    .sort(col('Total'), ascending=False)\\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f712ce0b-b562-448c-84ea-3f826f8306af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####  QUESTION 4: Show the top 10 regions with the biggest percentage decrease in population, for the years 1990-2000 #####\n",
    "\n",
    "pop_data_raw = spark.read.options(header=True, inferSchema=True)\\\n",
    "    .csv(POP_PATH)\n",
    "\n",
    "# first column name is Null in file, find col name through columns attribute and rename first column to region\n",
    "cols = pop_data_raw.columns \n",
    "pop_data_clean_header = pop_data_raw.select(col(cols[0]).alias('Region'), col('1990'), col('2000') )\n",
    "\n",
    "# remove regions with no population data (e.g. Antarctica, Wake Island, Croatia, Former U.S.S.R.)\n",
    "pop_data_clean_pop_nums = pop_data_clean_header.filter((col('1990')  != 'NA') &  (col('2000')  != 'NA') )\n",
    "pop_data_clean_pop_nums = pop_data_clean_pop_nums.filter((col('1990')  != '--') &  (col('2000')  != '--') )\n",
    "\n",
    "# remove leftover aggregate regions\n",
    "pop_data_clean_regions = pop_data_clean_pop_nums\\\n",
    "    .filter((col('Region') != 'World') & (col('Region') != 'North America') & (col('Region') != 'Central & South America') & (col('Region') != 'Eurasia') & (col('Region') != 'Western Sahara') & (col('Region') != 'Asia & Oceania') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22854c04-f33b-490f-ba43-5011724e59f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n|      Region|       perc_increase|\n+------------+--------------------+\n|  Montserrat| -0.6318732525629077|\n|    Bulgaria|-0.12092718374010437|\n|Cook Islands|-0.11310494834148986|\n+------------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# calculate gross increase\n",
    "pop_decrease_gross = pop_data_clean_regions.withColumn('gross_increase', col('2000') - col('1990') )\n",
    "\n",
    "# remove regions with positive population growth\n",
    "pop_decrease_gross = pop_decrease_gross.filter(col('gross_increase') <= 0)\n",
    "\n",
    "# calculate percentage of growth increase\n",
    "pop_decrease_perc = pop_decrease_gross.withColumn('perc_increase', col('gross_increase')/col('1990') )\n",
    "\n",
    "# create DF sorted in ascending order of pop increase, only contains Region and perc_increase \n",
    "pop_q4_res = pop_decrease_perc.sort(col('perc_increase') )\\\n",
    "    .select(col('Region'), col('perc_increase') )\n",
    "\n",
    "# show top 3 results\n",
    "pop_q4_res.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b42a86-d1fe-45bf-926c-b705df5fa7d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 5 #####\n",
    "\"\"\"\n",
    "Do word count exercise using pyspark. Ignore punctuation and normalize to lower case. Replace characters NOT in this set: [0-9a-z] with space.\n",
    "\"\"\"\n",
    "word_df = spark.read.text(WORD_DIR_PATH)\n",
    "\n",
    "# normalize to lower case\n",
    "word_df_lower = word_df.select(lower(col('value')).alias('words') )\n",
    "\n",
    "# replace punctuation with space\n",
    "word_df_parsed = word_df_lower.withColumn('words', regexp_replace('words', '[^a-z0-9]', ' ') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c30b6e-1af8-413f-aa16-40c9d23fbf50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|       words|count|\n+------------+-----+\n|       trail|   57|\n|       those| 3409|\n|    medicare|   32|\n|        some| 4335|\n|         few| 1057|\n|   connected|  162|\n| herzegovina|    7|\n|   involving|   99|\n|    randomly|   10|\n|     clinics|   78|\n|       still| 2139|\n| transmitted|   92|\n|      travel| 1367|\n|vicissitudes|    1|\n|      online| 1357|\n|         wto|   17|\n|  paramedics|   26|\n|          07|   90|\n|   traveling|   97|\n|   recognize|   66|\n+------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# explode and overwrite words column to create row for each word\n",
    "word_df_explode = word_df_parsed.withColumn('words', explode(split('words', \" \") ) )\n",
    "\n",
    "# group by words and count each occurence of each word\n",
    "word_q5_res = word_df_explode.groupBy(col('words') ).count()\n",
    "\n",
    "# show result\n",
    "word_q5_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3f43d1-c099-42ac-9390-bcde5a033825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n|bigrams| count|\n+-------+------+\n|       |769824|\n|      p| 77053|\n|     p | 76093|\n|    the| 30312|\n|      s| 21879|\n| of the| 17436|\n|    and| 13404|\n| in the| 12777|\n|     h |  8928|\n|      h|  8903|\n+-------+------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 6: Find the 10 most common bigrams #####\n",
    "\n",
    "# convert strings to arrays of strings for NGram transformation\n",
    "word_df_str_arrays = word_df_parsed.withColumn('words', split('words', ' ') )\n",
    "\n",
    "# transform word_df to bigram_df using NGram feature\n",
    "bigram = NGram(n=2).setInputCol('words').setOutputCol('bigrams')\n",
    "bigram_df = bigram.transform(word_df_str_arrays)\n",
    "\n",
    "# remove unnecessary words column\n",
    "bigram_df = bigram_df.select(col('bigrams') )\n",
    "\n",
    "# explode bigrams so each has own separate row\n",
    "bigram_df = bigram_df.withColumn('bigrams', explode(col('bigrams') ) )\n",
    "\n",
    "# group by bigram and count occurences\n",
    "bigram_df = bigram_df.groupBy(col('bigrams') ).count()\n",
    "\n",
    "# sort on bigram count for q5 res\n",
    "bigram_q5_res = bigram_df.sort('count', ascending=False)\n",
    "\n",
    "# show top 10 bigrams\n",
    "bigram_q5_res.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce430b64-3979-4b65-ab1e-08b562fa527a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: '\\na) Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\\nb) With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\\n'"
     ]
    }
   ],
   "source": [
    "##### Question 7 #####\n",
    "\"\"\"\n",
    "a) Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\n",
    "b) With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\n",
    "\"\"\"\n",
    "\n",
    "# can find a using only csv file, then just save and use that coordinate as your center point, maybe pass it in as lit. No\n",
    "# need to join"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dh3382-hw2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
