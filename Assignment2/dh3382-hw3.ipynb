{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd1433e-46a6-4320-8d91-0ddcbea7321a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting haversine\n  Downloading haversine-2.8.0-py2.py3-none-any.whl (7.7 kB)\nInstalling collected packages: haversine\nSuccessfully installed haversine-2.8.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "# run if haversine not already installed\n",
    "# %pip install haversine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998fd923-3d97-4f77-9211-4fc7195b930e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, count, dayofweek, explode, format_number, hour, lit, lower, regexp_replace, row_number, size, split, sum, udf\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine as hs\n",
    "\n",
    "spark = SparkSession.builder.appName(\"dh3382-hw2\").getOrCreate()\n",
    "\n",
    "BAKERY_PATH = '/FileStore/tables/BreadBasket_DMS.csv'\n",
    "POP_PATH = '/FileStore/tables/populationbycountry19802010millions.csv'\n",
    "REST_PATH = '/FileStore/tables/Restaurants_in_Durham_County_NC.csv'\n",
    "REST_FORECLOSED_PATH = '/FileStore/tables/durham_nc_foreclosure_2006_2016.json'\n",
    "WORD_DIR_PATH = '/FileStore/tables/hw1text/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba09d1a8-cbf3-4c33-9b49-87e5780ef88b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Question 1: Show total number bought per item, per day, between 11am and 1pm\n",
    "\n",
    "Program first reads in the BreadBasket_DMS csv file and filters out all NONE entries before moving on to part 1-specific processing. It proceeds to filter out all transactions outside of the specified time range (11:00-13:00 inclusive), and then groups the result first by Item, then by Date, and creates a \"Quantity\" column by counting the entries. This results in a dataframe that show the total number of each item bought per day between 11:00 and 13:00 inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42dbdc2d-6585-496c-974b-9828bb5f258c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read and clean bakery data\n",
    "bakery_data = spark.read.options(header=True, inferSchema=True).csv(BAKERY_PATH)\n",
    "\n",
    "# remove NONE entries\n",
    "bakery_data = bakery_data.filter(col('Item') != 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f04f4d7-2566-4ebe-956b-7bcbc849121a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+\n|                Item|      Date|Quantity|\n+--------------------+----------+--------+\n|Afternoon with th...|2017-01-21|       2|\n|Afternoon with th...|2017-01-22|       1|\n|Afternoon with th...|2017-02-18|       1|\n|           Alfajores|2016-11-02|       1|\n|           Alfajores|2016-11-04|       1|\n|           Alfajores|2016-11-08|       3|\n|           Alfajores|2016-11-11|       3|\n|           Alfajores|2016-11-12|       3|\n|           Alfajores|2016-11-13|       1|\n|           Alfajores|2016-11-17|       5|\n|           Alfajores|2016-11-20|       4|\n|           Alfajores|2016-11-25|       2|\n|           Alfajores|2016-11-27|       2|\n|           Alfajores|2016-12-03|       1|\n|           Alfajores|2016-12-04|       2|\n|           Alfajores|2016-12-05|       1|\n|           Alfajores|2016-12-07|       1|\n|           Alfajores|2016-12-09|       1|\n|           Alfajores|2016-12-13|       1|\n|           Alfajores|2016-12-14|       2|\n+--------------------+----------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 1: Show the total number bought by item, per day, between 11AM and 1PM #####\n",
    "\n",
    "# filter out all transactions outside of specified time range (11:00-13:00 inclusive)\n",
    "bakery_data_lunch = bakery_data.filter(col('Time').between('11:00:00', '13:00:00'))\n",
    "\n",
    "# group by Item and Day purchased, then get total count of items per day purchased between 11:00-13:00\n",
    "bakery_q1_res = bakery_data_lunch.groupBy(col('Item'), col('Date')).count().withColumnRenamed('count', 'Quantity')\n",
    "\n",
    "#sort by item, date for readability\n",
    "bakery_q1_res = bakery_q1_res.sort('Item', 'Date')\n",
    "\n",
    "# DF with answer to question 1\n",
    "bakery_q1_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3db1a3-0610-4f36-8b9b-db72bc3cedd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Question 2: Show the top 3 (by qty) items bought by Daypart, by DayType\n",
    "\n",
    "The first cell (Cmd 7) defines the UDFs that will be used to create the columns Daypart (Weekend/Weekday) and Daytype (Morning/Afternoon/Night). It then uses these UDFs to add the columns to the bakery_data DF that already had the NONE entries removed. The next step groups this DF by Daypart, Daytime, and Item to count the number of each item purchased for every possible Daytime/Daypart combination. A window function is then used to partition by Daypart/Daytime and order by total purchases per item, with the output limited to the top 3 purchases for each Daypart/Daytime. The last two transformations modify the format so that the top 3 items per Daypart/Daytime are collected in a list and displayed on a single row, and the final transformation reorders the columns for readability before calling show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f13d60-b5dd-402b-93fb-d60745d5cce5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 2: Show the top 3 (by qty) items bought by Daypart, by DayType #####\n",
    "\n",
    "#udf to define weekend/weekday\n",
    "def weekend(is_weekend):\n",
    "    if is_weekend:\n",
    "        return \"Weekend\"\n",
    "    else:\n",
    "        return \"Weekday\"\n",
    "\n",
    "weekend_udf = udf(lambda is_weekend: weekend(is_weekend), StringType() )\n",
    "\n",
    "# udf to define part of day\n",
    "def day_part(date):\n",
    "    if (date.hour >= 7) & (date.hour < 12):\n",
    "        return \"Morning\"\n",
    "    elif (date.hour >= 12) & (date.hour < 17):\n",
    "        return \"Afternoon\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "day_part_udf = udf(lambda date: day_part(date), StringType() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a12208-7089-45b1-a30e-e11b6615907b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------+-------+\n|Daypart  |Top_3_Items                               |Daytype|\n+---------+------------------------------------------+-------+\n|Afternoon|[Coffee, Bread, Tea]                      |Weekday|\n|Afternoon|[Coffee, Bread, Tea]                      |Weekend|\n|Morning  |[Coffee, Bread, Pastry]                   |Weekday|\n|Morning  |[Coffee, Bread, Pastry]                   |Weekend|\n|Night    |[Coffee, Bread, Tea]                      |Weekday|\n|Night    |[Coffee, Tshirt, Afternoon with the baker]|Weekend|\n+---------+------------------------------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# use weekend udf to add Daytype column, pass in boolean with sparkSQL dayofweek and isin functions\n",
    "bakery_weekend = bakery_data.withColumn('Daytype', weekend_udf(dayofweek(col('Date') ).isin([1, 7]) ) )\n",
    "\n",
    "# use day_part udf to add Daypart column\n",
    "bakery_daypart = bakery_weekend.withColumn('Daypart', day_part_udf(col('Time') ) )\n",
    "\n",
    "#group by Daypart and Daytype and count purchases\n",
    "bakery_purchases = bakery_daypart.groupBy(col('Daypart'), col(\"Daytype\"), col('Item') )\\\n",
    "  .count()\\\n",
    "  .withColumnRenamed('count', 'Purchases')\n",
    "\n",
    "# partition by Daypart, Daytype and find top 3 Purchases for each Daypart/Daytime combination\n",
    "windowBakery = Window.partitionBy('Daypart', 'Daytype')\\\n",
    "  .orderBy(col('Purchases').desc() )\n",
    "\n",
    "bakery_top_purchases = bakery_purchases.withColumn(\"row\",row_number()\\\n",
    "  .over(windowBakery) ) \\\n",
    "  .filter(col(\"row\") <= 3) \\\n",
    "\n",
    "# Collect all items into single row grouped by Daypart/Daytype\n",
    "bakery_q2_res = bakery_top_purchases.groupBy(col('Daypart'), col(\"Daytype\") )\\\n",
    "  .agg(collect_list('Item').alias('Top_3_Items') )\n",
    "\n",
    "# Select columns in order Daypart/Top_3_Items/Daytype for cleaner output format\n",
    "bakery_q2_res.select(col('Daypart'), col('Top_3_Items'), col('Daytype') ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1fb0265-8ab1-4800-81f1-e9ef81c1ad19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Question 3: The total number of entities by “rpt_area_desc”\n",
    "\n",
    "The first cell (Cmd 10) reads in the European style CSV file, defining the semicolon delimiter in read options. After dropping any entries with a null Rpt_Area_desc entry, the resulting DF is then simply grouped by the Rpt_Area_Desc column, counted to a column 'Total', and sorted in descending order. The final DF is then shown with the top 20 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f934241-835d-4c57-ae74-65c5f225b7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 3: The total number of entities by “rpt_area_desc” #####\n",
    "# set European style csv delimiter (;)\n",
    "rest_data_raw = spark.read.options(header=True, delimiter=';', inferSchema=True)\\\n",
    "    .csv(REST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083bd6c0-d148-4e1b-a589-f1bebf6d09ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|       Rpt_Area_Desc|Total|\n+--------------------+-----+\n|  Bed&Breakfast Home|    4|\n|        Summer Camps|    4|\n|        Institutions|   30|\n|   Local Confinement|    2|\n|         Mobile Food|  147|\n|    School Buildings|   89|\n|         Summer Food|  242|\n|      Swimming Pools|  420|\n|            Day Care|  173|\n|Tattoo Establishm...|   32|\n|    Residential Care|  154|\n|   Bed&Breakfast Inn|    2|\n|      Adult Day Care|    5|\n|             Lodging|   62|\n|        Food Service| 1093|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# group by rpt area desc, remove null entries, and aggregate count as 'Total'\n",
    "rest_q3_res = rest_data_raw\\\n",
    "    .na.drop(subset=['Rpt_Area_Desc'])\\\n",
    "    .groupBy(col('Rpt_Area_Desc') )\\\n",
    "    .agg(count('*').alias('Total') )\\\n",
    "\n",
    "rest_q3_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a8611c0-12ab-4fea-bb69-83e98aaa2536",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Question 4: Show the top 10 regions with the biggest percentage decrease in population, for the years 1990-2000\n",
    "\n",
    "The first cell (Cmd 13) is dedicated to data cleanup. The column name for region was missing from the header, so the columns attribute is used to find the default name and rename the first column to 'Region'. Some entries included either 'NA' or '--' population data, so these entries were filtered out to leave only entries with valid population numbers. Filling in a random value when one was not available did not seem reasonable, and '--' seems to apply to countries and regions that no longer exist. If a country did not exist for the entire range 1990-2000, it does not seem reasonable to assert it had a population decrease during that specific range. Furthermore, there were 7 aggregate regions listed. The Former USSR was already removed by the previous filter, but the other 6 had to be manually filtered out to leave only country-level data as specified during class.\n",
    "\n",
    "After cleaning, I first calculated the gross increase for each region and created a new column 'gross_increase'. I then filtered out all positive population increases as they would not be among the top 10 population decreases, and this could potentially save significant compute time on a larger dataset. I then calculated the percent increases, saved them in a column 'perc_increase_1990_2000', and sorted the data on 'perc_increase_1990_2000' while saving only the Region and perc_increase_1990_2000 columns to a new dataframe. Note that a negative perc_increase denotes a population decrease. For the final result, I showed the top 10 results from this sorted DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f712ce0b-b562-448c-84ea-3f826f8306af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####  QUESTION 4: Show the top 10 regions with the biggest percentage decrease in population, for the years 1990-2000 #####\n",
    "\n",
    "pop_data_raw = spark.read.options(header=True, inferSchema=True)\\\n",
    "    .csv(POP_PATH)\n",
    "\n",
    "# first column name is Null in file, find col name through columns attribute and rename first column to region\n",
    "cols = pop_data_raw.columns \n",
    "pop_data_clean_header = pop_data_raw.select(col(cols[0]).alias('Region'), col('1990'), col('2000') )\n",
    "\n",
    "# remove regions with no population data (e.g. Antarctica, Wake Island, Croatia, Former U.S.S.R.)\n",
    "pop_data_clean_pop_nums = pop_data_clean_header.filter((col('1990')  != 'NA') &  (col('2000')  != 'NA') )\n",
    "pop_data_clean_pop_nums = pop_data_clean_pop_nums.filter((col('1990')  != '--') &  (col('2000')  != '--') )\n",
    "\n",
    "# remove leftover aggregate regions\n",
    "pop_data_clean_regions = pop_data_clean_pop_nums\\\n",
    "    .filter((col('Region') != 'World') & (col('Region') != 'North America') & (col('Region') != 'Central & South America') & (col('Region') != 'Eurasia') & (col('Region') != 'Western Sahara') & (col('Region') != 'Asia & Oceania') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22854c04-f33b-490f-ba43-5011724e59f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------+\n|       Region|Percent Increase (1990-2000)|\n+-------------+----------------------------+\n|   Montserrat|                      -63.19|\n|     Bulgaria|                      -12.09|\n| Cook Islands|                      -11.31|\n| Sierra Leone|                       -9.91|\n|       Kuwait|                       -7.84|\n|    Gibraltar|                       -6.32|\n|Faroe Islands|                       -3.48|\n|      Albania|                       -2.67|\n|      Hungary|                       -2.16|\n|      Romania|                       -1.83|\n+-------------+----------------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# calculate gross increase\n",
    "pop_decrease_gross = pop_data_clean_regions.withColumn('gross_increase', col('2000') - col('1990') )\n",
    "\n",
    "# remove regions with positive population growth\n",
    "pop_decrease_gross = pop_decrease_gross.filter(col('gross_increase') <= 0)\n",
    "\n",
    "# calculate percentage of growth increase, multiply float by 100 for readability\n",
    "pop_decrease_perc = pop_decrease_gross.withColumn('perc_increase_1990_2000', (col('gross_increase')/col('1990') ) * 100 )\n",
    "\n",
    "# create DF sorted in ascending order of pop increase, only contains Region and Percent Increase, formatted to 2 decimal\n",
    "# points for readability\n",
    "pop_q4_res = pop_decrease_perc.sort(col('perc_increase_1990_2000') )\\\n",
    "    .select(col('Region'), format_number(col('perc_increase_1990_2000'), 2 ).alias('Percent Increase (1990-2000)') )\n",
    "\n",
    "# show top 10 results\n",
    "pop_q4_res.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3465b71a-f7b0-4a4e-803f-b87258dcc365",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 5: Word Count\n",
    "\n",
    "In the first cell (Cmd 16), I first read in the data and normalize the words to lower case, replacing all punctuation with white spaces. I then split the words values by spaces to create an array of single words and explode the column to create a DF where every single word from the input has its own row. I overwrite the original words column to avoid duplicate data when exploding. I then simply group by the words column and call count to create a new DF with all unique words and their respective number of occurrences ('count') and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b42a86-d1fe-45bf-926c-b705df5fa7d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### QUESTION 5 #####\n",
    "\"\"\"\n",
    "Do word count exercise using pyspark. Ignore punctuation and normalize to lower case. Replace characters NOT in this set: [0-9a-z] with space.\n",
    "\"\"\"\n",
    "word_df = spark.read.text(WORD_DIR_PATH)\n",
    "\n",
    "# normalize to lower case\n",
    "word_df_lower = word_df.select(lower(col('value')).alias('words') )\n",
    "\n",
    "# replace punctuation with space\n",
    "word_df_parsed = word_df_lower.withColumn('words', regexp_replace('words', '[^a-z0-9]', ' ') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c30b6e-1af8-413f-aa16-40c9d23fbf50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|       words|count|\n+------------+-----+\n|       trail|   57|\n|       those| 3409|\n|    medicare|   32|\n|        some| 4335|\n|         few| 1057|\n|   connected|  162|\n| herzegovina|    7|\n|   involving|   99|\n|    randomly|   10|\n|     clinics|   78|\n|       still| 2139|\n| transmitted|   92|\n|      travel| 1367|\n|vicissitudes|    1|\n|      online| 1357|\n|         wto|   17|\n|  paramedics|   26|\n|          07|   90|\n|   traveling|   97|\n|   recognize|   66|\n+------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# explode and overwrite words column to create row for each word\n",
    "word_df_explode = word_df_parsed.withColumn('words', explode(split('words', \" +\") ) )\n",
    "\n",
    "# group by words and count each occurence of each word\n",
    "word_q5_res = word_df_explode.groupBy(col('words') ).count()\n",
    "\n",
    "# show result\n",
    "word_q5_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3cf2b21-0566-446f-95ac-c237d621ab6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 6: 10 Most Common Bigrams\n",
    "\n",
    "In the first cell (Cmd 19), I transform the parsed words column, which was normalized and had its punctuation removed but had not yet been exploded, and convert each row into an array of strings using 'split'. This is a requirement for using the NGram feature from the pyspark.ml.feature library. I define the bigram transformation, as well as the input and output columns, before applying the transformation on this DF. The result is a DF where all rows are an array of bigrams as well as the original array of word strings. I remove the 'words' column to avoid unnecessary duplicate data before exploding the bigram column to obtain a DF where each row is a bigram. I then group by bigram and count in order to count the occurence of each unique bigrams in the DF. After this, I sort the resulting DF by the count column in descending order and show the first 10 columns to provide the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3f43d1-c099-42ac-9390-bcde5a033825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n| bigrams|count|\n+--------+-----+\n|  of the|17484|\n|  in the|12808|\n|   p the|10363|\n|covid 19| 8762|\n|  to the| 8372|\n| for the| 5588|\n|     n t| 5393|\n|  on the| 5032|\n|   to be| 4581|\n| will be| 4177|\n+--------+-----+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "##### QUESTION 6: Find the 10 most common bigrams #####\n",
    "\n",
    "# convert strings to arrays of strings for NGram transformation\n",
    "word_df_str_arrays = word_df_parsed.withColumn('words', split('words', ' +') )\n",
    "\n",
    "# transform word_df to bigram_df using NGram feature\n",
    "bigram = NGram(n=2).setInputCol('words').setOutputCol('bigrams')\n",
    "bigram_df = bigram.transform(word_df_str_arrays)\n",
    "\n",
    "# remove unnecessary words column\n",
    "bigram_df = bigram_df.select(col('bigrams') )\n",
    "\n",
    "# explode bigrams so each has own separate row\n",
    "bigram_df = bigram_df.withColumn('bigrams', explode(col('bigrams') ) )\n",
    "\n",
    "# group by bigram and count occurences\n",
    "bigram_df = bigram_df.groupBy(col('bigrams') ).count()\n",
    "\n",
    "# sort on bigram count for q5 res\n",
    "bigram_q5_res = bigram_df.sort('count', ascending=False)\n",
    "\n",
    "# show top 10 bigrams\n",
    "bigram_q5_res.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebf70b4a-a859-4c05-b671-71ccd937785f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 7: Geolocations\n",
    "\n",
    "I first defined a UDF to use in both parts a and b. It takes in two columns of coordinate strings, converts the strings to float tuples and passes them to the haversine function with the units defined as miles. It returns a column of the haversine distance between the two coordinates in miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce430b64-3979-4b65-ab1e-08b562fa527a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Question 7 #####\n",
    "\"\"\"\n",
    "a) Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\n",
    "b) With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\n",
    "\"\"\"\n",
    "# user function that accepts 2 strings of coordinates and outputs float distance in miles for part a\n",
    "def hs_with_str(coord_str1, coord_str2):\n",
    "    # convert coordinate strings to float tuples\n",
    "    coord_tup1 = tuple(map(float, coord_str1.split(', ') ) )\n",
    "    coord_tup2 = tuple(map(float, coord_str2.split(', ') ) )\n",
    "\n",
    "    # plug float tuples intro haversine function, specify unit as miles\n",
    "    return hs(coord_tup1, coord_tup2, unit='mi')\n",
    "\n",
    "# convert hs_with_str to udf\n",
    "hs_with_str_udf = udf(lambda coord1, coord2: hs_with_str(coord1, coord2), FloatType() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5486832e-4311-4226-aeac-606057a0d6c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part A: Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\n",
    "\n",
    "I first filter the rest_data_raw DF from Q3 to define a new DF with only the active, food service restaurants. I then drop any rows with null geolocation values, as these null values would break the haversine UDF and there is no reasonable replacement data in this case. I then use the haversine UDF to create a new column, 'Distance(mi)', passing in a column of the origin coordinates using the lit() function as well as the geolocation column. I obtain the final results by sorting this DF, and then show the top result with only the columns 'Premise_name', 'Status', 'Rpt_Area_Desc', 'geolocation', and 'Distance(mi)' for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f02bdc2-e49d-46f5-8a78-94b8de328e82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------+-------------+-----------------------+------------+\n|Premise_Name            |Status|Rpt_Area_Desc|geolocation            |Distance(mi)|\n+------------------------+------+-------------+-----------------------+------------+\n|OLD HAVANA SANDWICH SHOP|ACTIVE|Food Service |35.9932826, -78.8981331|0.1258222   |\n+------------------------+------+-------------+-----------------------+------------+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "## PART A: Find the food service and active restaurant closest to the following coordinates: 35.994914, -78.897133\n",
    "\n",
    "# origin coordinate string specified in prompt\n",
    "ORIGIN_COORD_STR = '35.994914, -78.897133'\n",
    "\n",
    "#filter dataset to include only active food service restaurants\n",
    "rest_data_active_fs = rest_data_raw.filter(col('Status') == 'ACTIVE').filter(col('Rpt_Area_Desc') == 'Food Service')\n",
    "\n",
    "# drop any restaurants with no geolocation data\n",
    "rest_data_active_fs = rest_data_active_fs.na.drop(subset=['geolocation'])\n",
    "\n",
    "# apply haversine UDF to create Distance(mi) column\n",
    "rest_data_distance = rest_data_active_fs\\\n",
    "    .withColumn('Distance(mi)', hs_with_str_udf(col('geolocation'), lit(ORIGIN_COORD_STR) ) )\n",
    "\n",
    "# sort on Distance(mi) for final result\n",
    "rest_q7a_res = rest_data_distance.sort('Distance(mi)')\n",
    "\n",
    "# show closest restaurant, select only relevant columns for cleaner output\n",
    "rest_q7a_res\\\n",
    "    .select(col('Premise_Name'), col('Status'), col('Rpt_Area_Desc'), col('geolocation'), col('Distance(mi)') )\\\n",
    "    .show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce267b3-b49e-471e-ba6f-ccb0f0429ad8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Part B: With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\n",
    "\n",
    "I save the coordinates from the part a's result using collect(), and then read in the json dataset. I create a DF with only the geocode subfield from this dataset, as that is all that is required to answer the question. I again remove rows with null values for coordinates. Because my original haversine UDF accepts strings, I cast the geocode column, originally a list of floats, to a string column and remove the brackets. The list values would require some casting either way to be able to use the haversine function, so this seemed to be an acceptable, though somewhat bulky, solution. I again create a 'Distance(mi)' column in the same way as part A, and proceed to filter out all distances over 1 mile. For the final result, I simply print out the count of the resulting DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cf25b5-63f0-4865-83fd-8b192d508e47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT\nNumber of foreclosures within 1 mile of center point: 320\n"
     ]
    }
   ],
   "source": [
    "## PART B: With that restaurant as your center point, find the number of foreclosures within a 1 mile radius\n",
    "\n",
    "# save coordinates of restaurant from Q7a to variable COORD_STR\n",
    "CENTER_COORD_STR = rest_q7a_res.collect()[0]['geolocation']\n",
    "\n",
    "# read in foreclosure dataset\n",
    "fc_rest_raw = spark.read.json(REST_FORECLOSED_PATH)\n",
    "\n",
    "# select only geocode subfield and discard unnecessary data\n",
    "fc_rest_coords = fc_rest_raw.select(col('fields').getItem('geocode').alias('geocodes') )\n",
    "\n",
    "# remove entries with null geolocation values\n",
    "fc_rest_coords = fc_rest_coords.na.drop()\n",
    "\n",
    "# cast geocode column to string\n",
    "fc_coords_str = fc_rest_coords.withColumn('geocodes', col('geocodes').cast(StringType() ) )\n",
    "\n",
    "# remove brackets\n",
    "fc_coords_str = fc_coords_str.withColumn('geocodes', regexp_replace('geocodes', \"[\\[\\]]\", \"\") )\n",
    "\n",
    "# calculate Distance(mi) column using hs_with_str_udf\n",
    "fc_coords_distance = fc_coords_str.withColumn('Distance(mi)', hs_with_str_udf(col('geocodes'), lit(CENTER_COORD_STR) ) )\n",
    "\n",
    "#filter out all restaurants more than 1 mile from center point\n",
    "fc_coords_q7b_res = fc_coords_distance.filter(col('Distance(mi)') <= 1)\n",
    "\n",
    "# count and print umber of foreclosures within 1 mile of center point\n",
    "print(\"RESULT\")\n",
    "print('Number of foreclosures within 1 mile of center point: %d' % fc_coords_q7b_res.count() )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dh3382-hw3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
